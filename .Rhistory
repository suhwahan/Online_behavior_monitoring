Tau=tau_1_mat[p,j])
x <- resp[p, RG[1]:RG[2]]
pr <- irf_3pl_v(xi_ir=ir_est_mat[RG[1]:RG[2],], th=th_1_mat[p,j]) # IRF at the interim theta at j
f_x_num <- pr^x*(1-pr)^(1-x)
f_num <- prod(f_rt_num)*prod(f_x_num)
f_rt_den <- rt_dfn(rt=rt[j,RG[1]:RG[2]], # Calculate the lognormal RT density given the null tau
Alp=rt_est_mat[RG[1]:RG[2], "alpha"],
Bet=rt_est_mat[RG[1]:RG[2], "beta"],
Tau=tau_0_mat[p,j])
pr <- irf_3pl_v(xi_ir=ir_est_mat[RG[1]:RG[2],], th=th_0_mat[p,j]) # IRF at the null theta
f_x_den <- pr^x*(1-pr)^(1-x)
f_den <- prod(f_rt_den)*prod(f_x_den)
LR[j]<- log(f_num/f_den)
}
LR_mat[p,] <- LR
LR_mat[p,]
j=1
LR
LR <- vector(mode="numeric", length=neval)
if (sampling == "ref_mov_fixed"){
RG <- c(j+1, j+n_rf_s) # range of item indices within the item set at j
} else if (sampling == "exclusive"){
RG <- c((n_rf_s*j+1),(n_rf_s*j+n_rf_s)) # range of item indices within the item set at j
}
RG
f_rt_num
calc_SGLRT <- function(resp, rt, n_rf_s, ir_est_mat, rt_est_mat, mu_p_t, cov_p_t,
sampling){
## Input values for the function -----------------
# resp: a vector of item responses
# rt: a vector of response times
# n_rf_s: Number of items in the training set
# ir_est_mat: item parameters across the items
# rt_est_mat: response time parameters across the items
# mu_p_t: prior for the person parameters (mean ability and speed)
# cov_p_t: prior for the covariance matrix of the person parameters
# sampling: a character variable specifying sampling strategies
#     "ref_mov_fixed": Sliding window sampling with a fixed training set
#     "exclusive": Sampling with exclusive item batches with a fixed training set
# Note. The function was written to take the entire vector of item
#       responses and response times for the computational efficiency
#       Below the observations are evaluated sequentially
#       within this function
## Set up ---------------------------
Nitem <- ncol(resp)
Nexaminee <- nrow(resp)
if (sampling %in% c("ref_mov_fixed")){
st_eval <- n_rf_s+1; neval <- Nitem-st_eval+1
} else if (sampling == "exclusive"){
st_eval <- 2*n_rf_s; neval <- Nitem/n_rf_s-1
}
th_1_mat <- matrix(NA, nrow=Nexaminee, ncol=neval)
tau_1_mat <- matrix(NA, nrow=Nexaminee, ncol=neval)
## Obtain the trait estimates for different sampling strategies
# If sliding window sampling with a fixed training set
if (sampling == "ref_mov_fixed"){
## Expected theta and tau from a fixed training set -----
# The map function below is from Kang (2016)
th_tau_0 <- map(resp=resp[,1:n_rf_s], RT=rt[,1:n_rf_s],
ipar = cbind(ir_est_mat[1:n_rf_s,], rt_est_mat[1:n_rf_s,]),
ppar_prior=list(mu_p=mu_p_t, cov_p=cov_p_t),
SE=FALSE, D=1.702)
th_0_mat <- matrix(th_tau_0$est[,"th"], nrow=Nexaminee, ncol=neval)
tau_0_mat <- matrix(th_tau_0$est[,"tau"], nrow=Nexaminee, ncol=neval)
## Sequential theta and tau at j ------------------
for (j in 1:neval){
th_tau_1 <- map(resp=resp[,(j+1):(j+n_rf_s)], RT=rt[,(j+1):(j+n_rf_s)],
ipar = cbind(ir_est_mat[(j+1):(j+n_rf_s),],
rt_est_mat[(j+1):(j+n_rf_s),]),
ppar_prior=list(mu_p=mu_p_t, cov_p=cov_p_t),
SE=FALSE, D=1.702)
th_1_mat[,j] <- th_tau_1$est[,"th"]
tau_1_mat[,j] <- th_tau_1$est[,"tau"]
}
# If sampling with item batches with a fixed training set
} else if (sampling == "exclusive"){
## Expected theta and tau from a fixed training set -----
th_tau_0 <- map(resp=resp[,1:n_rf_s], RT=rt[,1:n_rf_s],
ipar=cbind(ir_est_mat[1:n_rf_s,], rt_est_mat[1:n_rf_s,]),
ppar_prior=list(mu_p=mu_p_t, cov_p=cov_p_t),
SE=FALSE, D=1.702)
th_0_mat <- matrix(th_tau_0$est[,"th"], nrow=Nexaminee, ncol=neval)
tau_0_mat <- matrix(th_tau_0$est[,"tau"], nrow=Nexaminee, ncol=neval)
## Sequential theta and tau at j ------------------
for (j in 1:neval){
th_tau_1 <- map(resp=resp[,(n_rf_s*j+1):(n_rf_s*j+n_rf_s)],
RT=rt[,(n_rf_s*j+1):(n_rf_s*j+n_rf_s)],
ipar=cbind(ir_est_mat[(n_rf_s*j+1):(n_rf_s*j+n_rf_s),],
rt_est_mat[(n_rf_s*j+1):(n_rf_s*j+n_rf_s),]),
ppar_prior=list(mu_p=mu_p_t, cov_p=cov_p_t),
SE=FALSE, D=1.702)
th_1_mat[,j] <- th_tau_1$est[,"th"]
tau_1_mat[,j] <- th_tau_1$est[,"tau"]
}
}
## Calculate the multivariate monitoring statistic ----------------
LR_mat <- matrix(NA, Nexaminee, neval)
for (p in 1:Nexaminee){
LR <- vector(mode="numeric", length=neval)
for (j in 1:neval){
if (sampling == "ref_mov_fixed"){
RG <- c(j+1, j+n_rf_s) # range of item indices within the item set at j
} else if (sampling == "exclusive"){
RG <- c((n_rf_s*j+1),(n_rf_s*j+n_rf_s)) # range of item indices within the item set at j
}
f_rt_num <- rt_dfn(rt=rt[p,RG[1]:RG[2]], # Calculate the lognormal RT density given tau at j
Alp=rt_est_mat[RG[1]:RG[2], "alpha"],
Bet=rt_est_mat[RG[1]:RG[2], "beta"],
Tau=tau_1_mat[p,j])
x <- resp[p, RG[1]:RG[2]]
pr <- irf_3pl_v(xi_ir=ir_est_mat[RG[1]:RG[2],], th=th_1_mat[p,j]) # IRF at the interim theta at j
f_x_num <- pr^x*(1-pr)^(1-x)
f_num <- prod(f_rt_num)*prod(f_x_num)
f_rt_den <- rt_dfn(rt=rt[p,RG[1]:RG[2]], # Calculate the lognormal RT density given the null tau
Alp=rt_est_mat[RG[1]:RG[2], "alpha"],
Bet=rt_est_mat[RG[1]:RG[2], "beta"],
Tau=tau_0_mat[p,j])
pr <- irf_3pl_v(xi_ir=ir_est_mat[RG[1]:RG[2],], th=th_0_mat[p,j]) # IRF at the null theta
f_x_den <- pr^x*(1-pr)^(1-x)
f_den <- prod(f_rt_den)*prod(f_x_den)
LR[j]<- log(f_num/f_den)
}
LR_mat[p,] <- LR
}
return(list(LR_mat=LR_mat))
}
LR_null <- calc_SGLRT(resp=null_Data$response, rt=null_Data$rt, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat, mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling=sampling)
# Calculate likelihood-ratio statistics with data with aberrancy
LR_ab <- calc_SGLRT(resp=Ab_Data$Ab_response, rt=Ab_Data$Ab_RT, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat, mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling=sampling)
LR_null_mat <- tmp_n <- LR_null$LR_mat
LR_ab_mat <- tmp_a <- LR_ab$LR_mat
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=LR_null_mat)
##--| Evaluate the performance (power, Type I, ARL) -----------
res_G_excl <- Eval_performance(Proc="SGLRT", Res=LR_ab_mat, CV=CV, e_dt_pt=e_dt_pt, sampling=sampling,
c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
res_G_excl
sampling <- "ref_mov_fixed" # sliding door sampling with a fixed training data
# Calculate likelihood-ratio statistics with null data
LR_null <- calc_SGLRT(resp=null_Data$response, rt=null_Data$rt, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat, mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling=sampling)
# Calculate likelihood-ratio statistics with data with aberrancy
LR_ab <- calc_SGLRT(resp=Ab_Data$Ab_response, rt=Ab_Data$Ab_RT, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat, mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling=sampling)
# Extract the likelihood-ratio statistics
LR_null_mat <- tmp_n <- LR_null$LR_mat
LR_ab_mat <- tmp_a <- LR_ab$LR_mat
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=LR_null_mat)
##--| Evaluate the performance (power, Type I, ARL) -----------
res_G <- Eval_performance(Proc="SGLRT", Res=LR_ab_mat, CV=CV, sampling=sampling,
c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
##--| Arrange and store the results ---------------------------
res_G <- cbind(" ", as.data.frame(res_G)); colnames(res_G) <- c(".id", names(res_G[2:ncol(res_G)]))
Res_w_fts[[f]]$G <- res_G
res_G
res_G_W_list <- vector(mode="list", length=length(lambda_G))
names(res_G_W_list) <- paste0("lambda=",as.character(lambda_G))
##--| Exponentially weighted control chart with smoothing parameter l------
if (!is.null(lambda_G)){
for (l in 1:length(lambda_G)){
LR_null_mat[,1] <- (1-lambda_G[l])*0 + lambda_G[l]*tmp_n[,1]
LR_ab_mat[,1] <- (1-lambda_G[l])*0 + lambda_G[l]*tmp_a[,1]
for (k in 2:ncol(tmp_n)){
LR_null_mat[,k] <- (1-lambda_G[l])*LR_null_mat[,k-1] + lambda_G[l]*tmp_n[,k]
LR_ab_mat[,k] <- (1-lambda_G[l])*LR_ab_mat[,k-1] + lambda_G[l]*tmp_a[,k]
}
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=LR_null_mat)
##--| Evaluate the performance (power, Type I, ARL) ---------
res_G_W_list[[l]] <- Eval_performance(Proc="SGLRT", Res=LR_ab_mat, CV=CV, sampling=sampling,
c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
}
}
##--| Arrange and store the results
res_G_W_list <- plyr::ldply(res_G_W_list, data.frame)
Res_w_fts[[f]]$G_W <- res_G_W_list
res_G_W_list
##--| G statistic with the exclusive sampling----------------------
sampling <- "exclusive" # exclusive item-batch sampling with a fixed training data
LR_null <- calc_SGLRT(resp=null_Data$response, rt=null_Data$rt, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat, mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling=sampling)
# Calculate likelihood-ratio statistics with data with aberrancy
LR_ab <- calc_SGLRT(resp=Ab_Data$Ab_response, rt=Ab_Data$Ab_RT, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat, mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling=sampling)
# Extract the likelihood-ratio statistics
LR_null_mat <- tmp_n <- LR_null$LR_mat
LR_ab_mat <- tmp_a <- LR_ab$LR_mat
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=LR_null_mat)
##--| Evaluate the performance (power, Type I, ARL) -----------
res_G_excl <- Eval_performance(Proc="SGLRT", Res=LR_ab_mat, CV=CV, e_dt_pt=e_dt_pt, sampling=sampling,
c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
##--| Arrange and store the results ---------------------------
res_G_excl <- cbind(" ", as.data.frame(res_G_excl));
colnames(res_G_excl) <- c(".id", names(res_G_excl[2:ncol(res_G_excl)]))
Res_w_fts[[f]]$G_excl <- res_G_excl
res_G_excl
res_G_W_excl_list <- vector(mode="list", length=length(lambda_G_excl))
names(res_G_W_excl_list) <- paste0("lambda=",as.character(lambda_G_excl))
##--| Exponentially weighted control chart with smoothing parameter l------
if (!is.null(lambda_G_excl)){
for (l in 1:length(lambda_G_excl)){
LR_null_mat[,1] <- (1-lambda_G_excl[l])*0 + lambda_G_excl[l]*tmp_n[,1]
LR_ab_mat[,1] <- (1-lambda_G_excl[l])*0 + lambda_G_excl[l]*tmp_a[,1]
for (k in 2:ncol(tmp_n)){
LR_null_mat[,k] <- (1-lambda_G_excl[l])*LR_null_mat[,k-1] + lambda_G_excl[l]*tmp_n[,k]
LR_ab_mat[,k] <- (1-lambda_G_excl[l])*LR_ab_mat[,k-1] + lambda_G_excl[l]*tmp_a[,k]
}
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=LR_null_mat)
##--| Evaluate the performance (power, Type I, ARL) ---------
res_G_W_excl_list[[l]] <- calcPw_SGLRT(Res=LR_ab_mat, CV=CV, e_dt_pt_excl=e_dt_pt, sampling=sampling,
c.pt=c.pt, st_eval=LR_ab$st_eval, idx_ab_p=Ab_Data$idx_ab_p)
}
}
res_G_excl
res_G_W_excl_list <- vector(mode="list", length=length(lambda_G_excl))
names(res_G_W_excl_list) <- paste0("lambda=",as.character(lambda_G_excl))
##--| Exponentially weighted control chart with smoothing parameter l------
if (!is.null(lambda_G_excl)){
for (l in 1:length(lambda_G_excl)){
LR_null_mat[,1] <- (1-lambda_G_excl[l])*0 + lambda_G_excl[l]*tmp_n[,1]
LR_ab_mat[,1] <- (1-lambda_G_excl[l])*0 + lambda_G_excl[l]*tmp_a[,1]
for (k in 2:ncol(tmp_n)){
LR_null_mat[,k] <- (1-lambda_G_excl[l])*LR_null_mat[,k-1] + lambda_G_excl[l]*tmp_n[,k]
LR_ab_mat[,k] <- (1-lambda_G_excl[l])*LR_ab_mat[,k-1] + lambda_G_excl[l]*tmp_a[,k]
}
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=LR_null_mat)
##--| Evaluate the performance (power, Type I, ARL) ---------
res_G_W_excl_list[[l]] <- Eval_performance(Proc="SGLRT", Res=LR_ab_mat, CV=CV, e_dt_pt=e_dt_pt, sampling=sampling,
c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
}
}
##--| Arrange and store the results
res_G_W_excl_list <- plyr::ldply(res_G_W_excl_list, data.frame)
Res_w_fts[[f]]$G_W_excl <- res_G_W_excl_list
res_G_W_excl_list
##--| An empty vector of lists to store results from the simulation conditions
Res_w_fts <- vector(mode="list", length=nrow(sim_fts))
##--| A simulation instance for simulation condition f -------------
for (f in 1:nrow(sim_fts)){ # f: index of a simulation condition
# Specify simulation condition
n_rf_s <- sim_fts[f, "n_rf_s"] # number of items within training or evaluation set
delta_th <- sim_fts[f,"delta_th"] # change in the ability
delta_tau <- sim_fts[f,"delta_tau"] # change in the speed
##--| Generate data with aberrancy according to f -------------
Ab_Data <- genAbData(p_ab_p=p_ab_p, c.pt=c.pt,
delta_th=delta_th, delta_tau=delta_tau, ipars=null_Data$ipars, ppars=null_Data$ppars)
# Store aberrant data
Res_w_fts[[f]]$Ab_Data <- Ab_Data
##--| Calculate monitoring statistics for indicator-based control chart -----------------
# Calculate monitoring statistics with data with aberrancy
Wj_mat_ab <- calc_W_j_ref_fixed(resp=Ab_Data$Ab_response, rt=Ab_Data$Ab_RT, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat,
mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling_th_1 = "union_S_R")
# Calculate monitoring statistics with null data
Wj_mat_null <- calc_W_j_ref_fixed(resp=null_Data$response, rt=null_Data$rt, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat,
mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling_th_1 ="union_S_R")
# Extract monitoring statistics
Wj_ab <- Wj_mat_ab$Wj_mat
Wj_null <- Wj_mat_null$Wj_mat
##--| An empty vector of lists to store results across the different effect-size parameters
res_CX_list <- vector(mod="list", length=length(ref_v_W_j))
names(res_CX_list) <- paste0("ref_v=", as.character(ref_v_W_j))
##--| A simulation instance for effect size l -------------
for (l in 1:length(ref_v_W_j)){
##--| CUSUM control chart -----------
# Construct a CUSUM control chart with the null data
CUSUM_Wj_null <- cusumUp(Wj_null, ref_v=ref_v_W_j[l])
# Construct a CUSUM control chart with the data with aberrancy
CUSUM_Wj_ab <- cusumUp(Wj_ab, ref_v=ref_v_W_j[l])
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=CUSUM_Wj_null)
##--| Evaluate the performance (power, Type I, ARL)
res_CX_list[[l]] <- Eval_performance(Proc="CUSUM_obs", Res=CUSUM_Wj_ab, CV=CV,
c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
}
# Collapse the result lists and store the results
res_CX_list <- plyr::ldply(res_CX_list, data.frame)
Res_w_fts[[f]]$CX <- res_CX_list
##--| Weighted control chart that monitors indicator variables  ----------
tmp_a <- Wj_ab
tmp_n <- Wj_null
##--| An empty vector of lists to store results across the smoothing parameter ------
res_CX_W_list <- vector(mode = "list", length = length(lambda_Wj));
names(res_CX_W_list) <- paste0("lambda=",as.character(lambda_Wj))
##--| Exponentially weighted control chart with smoothing parameter l------
if (!is.null(lambda_Wj)) {
for (l in 1:length(lambda_Wj)){ # l=1
Wj_ab[,1] <- (1-lambda_Wj[l])*0 + lambda_Wj[l]*tmp_a[,1]
Wj_null[,1] <- (1-lambda_Wj[l])*0 + lambda_Wj[l]*tmp_n[,1]
for (k in 2:ncol(tmp_a)){
Wj_ab[,k] <- (1-lambda_Wj[l])*Wj_ab[,k-1] + lambda_Wj[l]*tmp_a[,k]
Wj_null[,k] <- (1-lambda_Wj[l])*Wj_null[,k-1] + lambda_Wj[l]*tmp_n[,k]
}
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=Wj_null)
##--| Evaluate the performance (power, Type I, ARL) ---------
res_CX_W_list[[l]] <- Eval_performance(Proc="CUSUM_obs", Res=Wj_ab, CV=CV,
c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
}
}
##--| Arrange and store the results ------------------------------------------------
res_CX_W_list <- plyr::ldply(res_CX_W_list, data.frame)
Res_w_fts[[f]]$CX_W <- res_CX_W_list
##--| Calculate monitoring statistics for trait-based control chart -----------------
# Calculate monitoring statistics with data with aberrancy
tmp_ab <- calc_V_j_CUSUM_ref_fixed(resp=Ab_Data$Ab_response, rt=Ab_Data$Ab_RT, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat,
mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p)
V_j_mat_ab <- tmp_ab$V_j_mat
th_0_ab <- tmp_ab$th_0; tau_0_ab <- tmp_ab$tau_0
th_1_mat_ab <- tmp_ab$th_1_mat; tau_1_mat_ab <- tmp_ab$tau_1_mat
# Calculate monitoring statistics with null data
tmp_null <- calc_V_j_CUSUM_ref_fixed(resp=null_Data$response, rt=null_Data$rt, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat,
mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p)
V_j_mat_null <- tmp_null$V_j_mat
th_0_null <- tmp_null$th_0; tau_0_null <- tmp_null$tau_0
th_1_mat_null <- tmp_null$th_1_mat; tau_1_mat_null <- tmp_ab$tau_1_mat
##--| Earliest possible detection point for the batch-wise sampling------------
e_dt_pt  <- c.pt/n_rf_s
if (e_dt_pt - floor(e_dt_pt) == 0 && e_dt_pt > 1){
e_dt_pt  <- e_dt_pt -1
} else if (e_dt_pt-floor(e_dt_pt) != 0 && e_dt_pt > 1){
e_dt_pt <- floor(e_dt_pt)
} else {
e_dt_pt  <- 1
}
##--| An empty vector of lists to store results across the different effect-size parameters
res_CE_list <- vector(mode="list", length=length(ref_v_V_j))
names(res_CE_list) <- paste0("ref_v=", as.character(ref_v_V_j))
##--| A simulation instance for effect size l -------------
for (l in 1:length(ref_v_V_j)){
##--| CUSUM control chart -----------
# Construct a CUSUM control chart with the null data
CUSUM_Vj_null <- cusumUp(V_j_mat_null, ref_v=ref_v_V_j[l])
# Construct a CUSUM control chart with the data with aberrancy
CUSUM_Vj_ab <- cusumUp(V_j_mat_ab, ref_v=ref_v_V_j[l])
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=CUSUM_Vj_null)
##--| Evaluate the performance (power, Type I, ARL)
res_CE_list[[l]] <- Eval_performance(Proc="CUSUM_est", Res=CUSUM_Vj_ab, CV=CV,
e_dt_pt=e_dt_pt, c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
}
# Arrange and store the results ----------------------
res_CE_list <- plyr::ldply(res_CE_list, data.frame)
Res_w_fts[[f]]$CE <- res_CE_list
##--| Weighted control chart that monitors latent trait variables ----------
tmp_a <- V_j_mat_ab
tmp_n <- V_j_mat_null
##--| An empty vector of lists to store results across the smoothing parameter ------
res_CE_W_list <- vector(mode="list", length=length(lambda_Vj))
names(res_CE_W_list) <- paste0("lambda=", as.character(lambda_Vj))
##--| Exponentially weighted control chart with smoothing parameter l------
if (!is.null(lambda_Vj)){
for (l in 1:length(lambda_Vj)){
V_j_mat_null[,1] <- (1-lambda_Vj[l])*0 + lambda_Vj[l]*tmp_n[,1]
V_j_mat_ab[,1] <- (1-lambda_Vj[l])*0 + lambda_Vj[l]*tmp_a[,1]
for (k in 2:ncol(tmp_a)){
V_j_mat_null[,k] <- (1-lambda_Vj[l])*V_j_mat_null[,k-1] + lambda_Vj[l]*tmp_n[,k]
V_j_mat_ab[,k] <- (1-lambda_Vj[l])*V_j_mat_ab[,k-1] + lambda_Vj[l]*tmp_a[,k]
}
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=V_j_mat_null)
##--| Evaluate the performance (power, Type I, ARL) ---------
res_CE_W_list[[l]] <- Eval_performance(Proc="CUSUM_est", Res=CUSUM_Vj_ab, CV=CV,
e_dt_pt=e_dt_pt, c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
}
}
##--| Arrange and store the results ----------------------------
res_CE_W_list <- plyr::ldply(res_CE_W_list, data.frame)
Res_w_fts[[f]]$CE_W <- res_CE_W_list
##--| SGLRT with the sliding door sampling----------------------
sampling <- "ref_mov_fixed" # sliding door sampling with a fixed training data
# Calculate likelihood-ratio statistics with null data
LR_null <- calc_SGLRT(resp=null_Data$response, rt=null_Data$rt, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat, mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling=sampling)
# Calculate likelihood-ratio statistics with data with aberrancy
LR_ab <- calc_SGLRT(resp=Ab_Data$Ab_response, rt=Ab_Data$Ab_RT, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat, mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling=sampling)
# Extract the likelihood-ratio statistics
LR_null_mat <- tmp_n <- LR_null$LR_mat
LR_ab_mat <- tmp_a <- LR_ab$LR_mat
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=LR_null_mat)
##--| Evaluate the performance (power, Type I, ARL) -----------
res_G <- Eval_performance(Proc="SGLRT", Res=LR_ab_mat, CV=CV, sampling=sampling,
c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
##--| Arrange and store the results ---------------------------
res_G <- cbind(" ", as.data.frame(res_G)); colnames(res_G) <- c(".id", names(res_G[2:ncol(res_G)]))
Res_w_fts[[f]]$G <- res_G
##--| Weighted SGLRT with the sliding door sampling-----------------------
##--| An empty vector of lists to store results across the smoothing parameter ------
res_G_W_list <- vector(mode="list", length=length(lambda_G))
names(res_G_W_list) <- paste0("lambda=",as.character(lambda_G))
##--| Exponentially weighted control chart with smoothing parameter l------
if (!is.null(lambda_G)){
for (l in 1:length(lambda_G)){
LR_null_mat[,1] <- (1-lambda_G[l])*0 + lambda_G[l]*tmp_n[,1]
LR_ab_mat[,1] <- (1-lambda_G[l])*0 + lambda_G[l]*tmp_a[,1]
for (k in 2:ncol(tmp_n)){
LR_null_mat[,k] <- (1-lambda_G[l])*LR_null_mat[,k-1] + lambda_G[l]*tmp_n[,k]
LR_ab_mat[,k] <- (1-lambda_G[l])*LR_ab_mat[,k-1] + lambda_G[l]*tmp_a[,k]
}
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=LR_null_mat)
##--| Evaluate the performance (power, Type I, ARL) ---------
res_G_W_list[[l]] <- Eval_performance(Proc="SGLRT", Res=LR_ab_mat, CV=CV, sampling=sampling,
c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
}
}
##--| Arrange and store the results
res_G_W_list <- plyr::ldply(res_G_W_list, data.frame)
Res_w_fts[[f]]$G_W <- res_G_W_list
##--| G statistic with the exclusive sampling----------------------
sampling <- "exclusive" # exclusive item-batch sampling with a fixed training data
# Calculate likelihood-ratio statistics with null data
LR_null <- calc_SGLRT(resp=null_Data$response, rt=null_Data$rt, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat, mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling=sampling)
# Calculate likelihood-ratio statistics with data with aberrancy
LR_ab <- calc_SGLRT(resp=Ab_Data$Ab_response, rt=Ab_Data$Ab_RT, n_rf_s=n_rf_s,
ir_est_mat=ir_est_mat, rt_est_mat=rt_est_mat, mu_p_t=null_Data$mu_p, cov_p_t=null_Data$cov_p,
sampling=sampling)
# Extract the likelihood-ratio statistics
LR_null_mat <- tmp_n <- LR_null$LR_mat
LR_ab_mat <- tmp_a <- LR_ab$LR_mat
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=LR_null_mat)
##--| Evaluate the performance (power, Type I, ARL) -----------
res_G_excl <- Eval_performance(Proc="SGLRT", Res=LR_ab_mat, CV=CV, e_dt_pt=e_dt_pt, sampling=sampling,
c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
##--| Arrange and store the results ---------------------------
res_G_excl <- cbind(" ", as.data.frame(res_G_excl));
colnames(res_G_excl) <- c(".id", names(res_G_excl[2:ncol(res_G_excl)]))
Res_w_fts[[f]]$G_excl <- res_G_excl
##--| Weighted SGLRT with the exclusive sampling ----------------------
res_G_W_excl_list <- vector(mode="list", length=length(lambda_G_excl))
names(res_G_W_excl_list) <- paste0("lambda=",as.character(lambda_G_excl))
##--| Exponentially weighted control chart with smoothing parameter l------
if (!is.null(lambda_G_excl)){
for (l in 1:length(lambda_G_excl)){
LR_null_mat[,1] <- (1-lambda_G_excl[l])*0 + lambda_G_excl[l]*tmp_n[,1]
LR_ab_mat[,1] <- (1-lambda_G_excl[l])*0 + lambda_G_excl[l]*tmp_a[,1]
for (k in 2:ncol(tmp_n)){
LR_null_mat[,k] <- (1-lambda_G_excl[l])*LR_null_mat[,k-1] + lambda_G_excl[l]*tmp_n[,k]
LR_ab_mat[,k] <- (1-lambda_G_excl[l])*LR_ab_mat[,k-1] + lambda_G_excl[l]*tmp_a[,k]
}
##--| Obtain decision limit (i.e., critical value) -----------
CV <- calc_CV(pct=.05, stats=LR_null_mat)
##--| Evaluate the performance (power, Type I, ARL) ---------
res_G_W_excl_list[[l]] <- Eval_performance(Proc="SGLRT", Res=LR_ab_mat, CV=CV, e_dt_pt=e_dt_pt, sampling=sampling,
c.pt=c.pt, n_rf_s=n_rf_s, idx_ab_p=Ab_Data$idx_ab_p)
}
}
##--| Arrange and store the results
res_G_W_excl_list <- plyr::ldply(res_G_W_excl_list, data.frame)
Res_w_fts[[f]]$G_W_excl <- res_G_W_excl_list
##--| Fox & Marianti (2017) ---------------------------
mod_hf <- LNIRT::LNIRT(RT=log(Ab_Data$Ab_RT), Y=Ab_Data$Ab_response, data=list(log(Ab_Data$Ab_RT), Ab_Data$Ab_response),
alpha=mod_LNIRT$Post.Means$Item.Discrimination,
beta=mod_LNIRT$Post.Means$Item.Difficulty,
phi=mod_LNIRT$Post.Means$Sigma2,
lambda=mod_LNIRT$Post.Means$Time.Intensity,
XG=3000, burnin=10,
guess=FALSE, par1=TRUE, td=TRUE, WL=TRUE, residual=TRUE)
##--| Arrange and store the results ------------------------
FM <- list()
FM$RT <- as.data.frame(calc_pfcase(nexaminee=nexaminee, trueAb=Ab_Data$idx_ab_p, estdAb=which(mod_hf$EAPCP1 > .95))) # Based on RT
FM$RA <- as.data.frame(calc_pfcase(nexaminee=nexaminee, trueAb=Ab_Data$idx_ab_p, estdAb=which(mod_hf$EAPCP2 > .95))) # Based on RA
FM$Both <- as.data.frame(calc_pfcase(nexaminee=nexaminee, trueAb=Ab_Data$idx_ab_p, estdAb=which(mod_hf$EAPCP3 > .95))) # Based on Both
FM$Union <- as.data.frame(calc_pfcase(nexaminee=nexaminee, trueAb=Ab_Data$idx_ab_p,
estdAb=unique(c(which(mod_hf$EAPCP1 > .95), which(mod_hf$EAPCP2 > .95))) ))
Res_w_fts[[f]]$FM <- plyr::ldply(FM, data.frame)
} # end of f
View(Res_w_fts)
source(SimulationDriver_Mar2023_Git.R)
source("SimulationDriver_Mar2023_Git.R")
library(foreach)
library(doParallel)
n.cores <- detectCores() - 2
cl <- makeCluster(n.cores)
registerDoParallel(cl)
sim <- foreach(r=c(1:2), .packages = c("LNIRT", "purrr", "plyr"), .errorhandling = "pass") %dopar% {
Sim_driver (nitem=40, nexaminee=1000, rho_item=.25,
rho_person=.3, c_f=0, p_ab_p=.1, c.pt=21,
mu_a=.91, var_a=.79^2, mu_alp=1.49, var_alp=.39^2,
ref_v_W_j = c(.25, .5, 1, 1.5, 2, 2.5, 3, 3.5, 4),
lambda_Wj = seq(0.005, 0.995, by = 0.005),
ref_v_V_j = c(0.5, 1, 2.5, 5, 7.5, 10, 12.5, 15),
lambda_Vj = seq(0.005, 0.995, by = 0.005),
lambda_G = seq(0.005, 0.995, by = 0.005),
lambda_G_excl = seq(0.005, 0.995, by = 0.005) )
}
View(sim)
View(sim)
